{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d877a6-0f97-497f-b4e2-7f4da0bc236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import config\n",
    "import json\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc415b2-70d6-41b8-97fd-574424aa1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup chromedriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# open the webpage\n",
    "driver.get(\"https://www.instagram.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ab5fcc-9dab-4c42-ad11-456bc127e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target username and password\n",
    "username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='username']\")))\n",
    "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='password']\")))\n",
    "\n",
    "# enter username and password\n",
    "username.clear()\n",
    "username.send_keys(config.username)\n",
    "password.clear()\n",
    "password.send_keys(config.password)\n",
    "\n",
    "# target and click login button\n",
    "button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a385708-e486-4163-8d49-081dfcf51177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait 10 sec for search button\n",
    "search_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'svg[aria-label=\"Pesquisa\"]')))\n",
    "\n",
    "# click search button\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a658023-995d-48a1-bc14-b3885403be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target searchbox\n",
    "searchbox = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'input[aria-label=\"Entrada da pesquisa\"]')))\n",
    "searchbox.clear()\n",
    "\n",
    "# search for @target\n",
    "keyword = config.target\n",
    "searchbox.send_keys(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9edbda98-a109-4102-a155-293fc80477e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click target\n",
    "time.sleep(5)\n",
    "result = driver.find_element(By.XPATH, f'//span[text()=\"{keyword}\"]')\n",
    "result.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b00ce319-f498-4847-9d7d-b3652e275bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the initial page height\n",
    "initial_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "# create a list to store htmls\n",
    "soups = []\n",
    "\n",
    "# scroll loop\n",
    "while True:\n",
    "\n",
    "    # scroll down to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # wait for a moment to allow new content to load (adjust as needed)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # parse the html\n",
    "    html = driver.page_source\n",
    "\n",
    "    # create a beautifulsoup object from the scraped html\n",
    "    soups.append(BeautifulSoup(html, 'html.parser'))\n",
    "\n",
    "    # get the current page height\n",
    "    current_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    if current_height == initial_height:\n",
    "        break # exit the loop when you cant scroll further\n",
    "\n",
    "    # update the initial heightfor the next iteration\n",
    "    initial_height = current_height\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4469cce0-c45d-4a6b-b6bc-bdd01df28384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to store the post image urls\n",
    "post_urls = []\n",
    "\n",
    "# loop through soup elements\n",
    "for soup in soups:\n",
    "\n",
    "    #find all image elements that match the specific class in the current soup\n",
    "    elements = soup.find_all('a', class_='x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz _a6hd')\n",
    "\n",
    "    # extract the href attributes and filterurls that start with \"/p/\" or \"/reel/\"\n",
    "    post_urls.extend([element['href'] for element in elements if element['href'].startswith((\"/p/\", \"/reel/\"))])\n",
    "\n",
    "# convert the list to a set to remove duplicates\n",
    "unique_post_urls = list(set(post_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4627630-ee28-4ec1-a3b0-84ef0b9175f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to store the json for each post\n",
    "json_list = []\n",
    "\n",
    "# define the query parameters to add\n",
    "query_parameters = \"?__a=1&__d=dis\"\n",
    "\n",
    "# go through all urls\n",
    "for url in unique_post_urls:\n",
    "\n",
    "    # error handling\n",
    "    try:\n",
    "\n",
    "        # get the current url of the page\n",
    "        current_url = driver.current_url\n",
    "\n",
    "        # append the query parameters to the current url\n",
    "        modified_url = \"https://instagram.com/\" + url + query_parameters\n",
    "\n",
    "        # get url\n",
    "        driver.get(modified_url)\n",
    "\n",
    "        # wait for a moment to allow new content to load\n",
    "        time.sleep(1)\n",
    "\n",
    "        # find the <pre> tag containing the json data\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//pre'))\n",
    "        )\n",
    "        pre_tag = driver.find_element(By.XPATH, '//pre')\n",
    "\n",
    "        # extract the json data from the <pre> tag\n",
    "        json_script = pre_tag.text\n",
    "\n",
    "        # parse the json data\n",
    "        json_parsed = json.loads(json_script)\n",
    "\n",
    "        # add json to the list\n",
    "        json_list.append(json_parsed)\n",
    "\n",
    "    # error handling\n",
    "    except (NoSuchElementException, TimeoutException, json.JSONDecodeError) as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce4af797-0027-4221-85eb-15846bdac367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single image added\n",
      "single video added\n",
      "single image added\n",
      "single video added\n",
      "single image added\n",
      "single video added\n",
      "single image added\n",
      "single video added\n",
      "carousel image added\n",
      "carousel image added\n",
      "single image added\n",
      "single image added\n",
      "single video added\n",
      "carousel image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "single image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "single image added\n",
      "single image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "single image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "carousel image added\n",
      "single image added\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# lists to store urls and corresponding dates\n",
    "all_urls = []\n",
    "all_dates = []\n",
    "\n",
    "# iterate through each json data in the list\n",
    "for json_data in json_list:\n",
    "\n",
    "    # extract the list from the 'items' key\n",
    "    item_list = json_data.get('items', [])\n",
    "\n",
    "    # iterate through each item in the 'items' list\n",
    "    for item in item_list:\n",
    "\n",
    "        # extract the date the item was taken\n",
    "        date_taken = json_data.get('taken_at')\n",
    "\n",
    "        # check if 'carousel_media' is present\n",
    "        carousel_media = item.get('carousel_media', [])\n",
    "\n",
    "        # iterate through each media in the 'carousel_media' list\n",
    "        for media in carousel_media:\n",
    "\n",
    "            # extract the image URL from the media\n",
    "            image_url = media.get('image_versions2', {}).get('candidates', [{}])[0].get('url')\n",
    "\n",
    "            # check if the image_url field id found inside the 'carousel_media' list\n",
    "            if image_url:\n",
    "\n",
    "                # add the image url and corresponding date to the lists\n",
    "                all_urls.append(image_url)\n",
    "                all_dates.append(date_taken)\n",
    "                print(\"carousel image added\")\n",
    "\n",
    "            # extract the video url from the media\n",
    "            video_versions = media.get('video_versions', [])\n",
    "            if video_versions:\n",
    "                video_url = video_versions[0].get('url')\n",
    "                if video_url:\n",
    "\n",
    "                    # add the video url and corresponding date to the list\n",
    "                    all_urls.append(video_url)\n",
    "                    all_dates.append(date_taken)\n",
    "                    print(\"carousel video added\")\n",
    "\n",
    "        # handle cases of unique image, instead of carousel\n",
    "        image_url = item.get('image_versions2', {}).get('candidates', [{}])[0].get('url')\n",
    "        if image_url:\n",
    "\n",
    "            # add the image url and corresponding date to the lists\n",
    "            all_urls.append(image_url)\n",
    "            all_dates.append(date_taken)\n",
    "            print(f\"single image added\")\n",
    "\n",
    "        # check if 'video_versions' key exists\n",
    "        video_versions = item.get('video_versions', [])\n",
    "        if video_versions:\n",
    "            video_url = video_versions[0].get('url')\n",
    "            if video_url:\n",
    "\n",
    "                # add the videourl and correspondig date to the lists\n",
    "                all_urls.append(video_url)\n",
    "                all_dates.append(date_taken)\n",
    "                print(f\"single video added\")\n",
    "\n",
    "print(len(all_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a739b4ef-fa69-43a9-8f12-d833ed03bddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27c2ae51-422d-45f1-8889-4fd9b31c1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: pharrell\\images\\None-img-1.png\n",
      "Downloaded: pharrell\\videos\\None-vid-1.mp4\n",
      "Downloaded: pharrell\\images\\None-img-2.png\n",
      "Downloaded: pharrell\\videos\\None-vid-2.mp4\n",
      "Downloaded: pharrell\\images\\None-img-3.png\n",
      "Downloaded: pharrell\\videos\\None-vid-3.mp4\n",
      "Downloaded: pharrell\\images\\None-img-4.png\n",
      "Downloaded: pharrell\\videos\\None-vid-4.mp4\n",
      "Downloaded: pharrell\\images\\None-img-5.png\n",
      "Downloaded: pharrell\\images\\None-img-6.png\n",
      "Downloaded: pharrell\\images\\None-img-7.png\n",
      "Downloaded: pharrell\\images\\None-img-8.png\n",
      "Downloaded: pharrell\\videos\\None-vid-5.mp4\n",
      "Downloaded: pharrell\\images\\None-img-9.png\n",
      "Downloaded: pharrell\\images\\None-img-10.png\n",
      "Downloaded: pharrell\\images\\None-img-11.png\n",
      "Downloaded: pharrell\\images\\None-img-12.png\n",
      "Downloaded: pharrell\\images\\None-img-13.png\n",
      "Downloaded: pharrell\\images\\None-img-14.png\n",
      "Downloaded: pharrell\\images\\None-img-15.png\n",
      "Downloaded: pharrell\\images\\None-img-16.png\n",
      "Downloaded: pharrell\\images\\None-img-17.png\n",
      "Downloaded: pharrell\\images\\None-img-18.png\n",
      "Downloaded: pharrell\\images\\None-img-19.png\n",
      "Downloaded: pharrell\\images\\None-img-20.png\n",
      "Downloaded: pharrell\\images\\None-img-21.png\n",
      "Downloaded: pharrell\\images\\None-img-22.png\n",
      "Downloaded: pharrell\\images\\None-img-23.png\n",
      "Downloaded: pharrell\\images\\None-img-24.png\n",
      "Downloaded: pharrell\\images\\None-img-25.png\n",
      "Downloaded: pharrell\\images\\None-img-26.png\n",
      "Downloaded: pharrell\\images\\None-img-27.png\n",
      "Downloaded: pharrell\\images\\None-img-28.png\n",
      "Downloaded: pharrell\\images\\None-img-29.png\n",
      "Downloaded: pharrell\\images\\None-img-30.png\n",
      "Downloaded: pharrell\\images\\None-img-31.png\n",
      "Downloaded 36 files to pharrell\n"
     ]
    }
   ],
   "source": [
    "# create a directory to store downloaded files\n",
    "download_dir = keyword\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "# create subfolders for images and videos\n",
    "image_dir = os.path.join(download_dir, \"images\")\n",
    "video_dir = os.path.join(download_dir, \"videos\")\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "# initialize counters for images and videos\n",
    "image_counter = 1\n",
    "video_counter = 1\n",
    "\n",
    "# iterate through urls in the all_urls list and download_media\n",
    "for index, url in enumerate(all_urls, 0):\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # extract file extension from the url\n",
    "    url_path = urlparse(url).path\n",
    "    file_extension = os.path.splitext(url_path)[1]\n",
    "\n",
    "    # determine the file name based on the url\n",
    "    if file_extension.lower() in {'.jpeg', '.jpg', '.png', '.gif'}:\n",
    "        file_name = f\"{all_dates[index]}-img-{image_counter}.png\"\n",
    "        destination_folder = image_dir\n",
    "        image_counter += 1\n",
    "    elif file_extension.lower() in {'.mp4', '.avi', '.mkv', '.mov'}:\n",
    "        file_name = f\"{all_dates[index]}-vid-{video_counter}.mp4\"\n",
    "        destination_folder = video_dir\n",
    "        video_counter += 1\n",
    "    else:\n",
    "        # default to the main donwload directory for other file types\n",
    "        file_name = f\"{all_dates[index]}{file_extesion}\"\n",
    "        destination_folder = download_dir\n",
    "\n",
    "    # save the file to the appropriate folder\n",
    "    file_path = os.path.join(destination_folder, file_name)\n",
    "\n",
    "    # write the content of the response to the file\n",
    "    with open(file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "\n",
    "    print(f\"Downloaded: {file_path}\")\n",
    "\n",
    "# print a message indicating the number of downloaded files and the download directory\n",
    "print(f'Downloaded {len(all_urls)} files to {download_dir}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
